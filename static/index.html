<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <title>English Academy AI - Streaming Pipeline</title>
    
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.19/dist/bundle.min.js"></script>

    <style>
        body { font-family: sans-serif; background: #121212; color: white; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        .card { background: #1e1e1e; padding: 40px; border-radius: 24px; text-align: center; width: 350px; border: 1px solid #333; }
        .indicator { width: 70px; height: 70px; border-radius: 50%; background: #333; margin: 20px auto; transition: 0.3s; border: 4px solid #121212; }
        .listening { background: #ff4b4b !important; box-shadow: 0 0 20px #ff4b4b; }
        .processing { background: #4b96ff !important; box-shadow: 0 0 20px #4b96ff; animation: pulse 1s infinite; }
        .speaking { background: #00e676 !important; box-shadow: 0 0 20px #00e676; animation: pulse 1s infinite; }
        @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } }
        button { padding: 15px 30px; border-radius: 50px; border: none; background: #00c853; color: white; font-weight: bold; cursor: pointer; width: 100%; }
        button:disabled { background: #444; cursor: not-allowed; }
        #status { margin-top: 20px; color: #aaa; font-size: 0.9rem; }
    </style>
</head>
<body>

<div class="card">
    <h1>Sarah AI</h1>
    <div id="indicator" class="indicator"></div>
    <p id="status">Cargando inteligencia...</p>
    <button id="startBtn" disabled>ESPERANDO SISTEMA...</button>
</div>

<script>
    const startBtn = document.getElementById('startBtn');
    const statusText = document.getElementById('status');
    const indicator = document.getElementById('indicator');

    // 1. Verificar carga de librer√≠as
    const checkLibs = setInterval(() => {
        if (typeof ort !== 'undefined' && typeof vad !== 'undefined') {
            clearInterval(checkLibs);
            startBtn.disabled = false;
            startBtn.innerText = "EMPEZAR CLASE";
            statusText.innerText = "Listo para conectar.";
        }
    }, 500);

    // 2. Conexi√≥n WebSocket
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = `${protocol}//${window.location.host}/ws`;
    console.log("üîå Conectando WebSocket a:", wsUrl);
    const ws = new WebSocket(wsUrl);

    ws.onopen = () => {
        console.log("‚úÖ WebSocket ABIERTO");
        statusText.innerText = "Conectado. Pulsa el bot√≥n.";
    };
    ws.onclose = () => {
        console.log("‚ùå WebSocket CERRADO");
        statusText.innerText = "Desconectado.";
    };
    ws.onerror = (error) => {
        console.error("‚ùå Error WebSocket:", error);
        statusText.innerText = "Error de conexi√≥n.";
    };

    // --- COLA DE AUDIO (Audio Queue) ---
    const audioQueue = [];
    let isPlaying = false;
    let isAiSpeaking = false; // Flag para controlar el estado de habla de la IA
    let currentAudio = null; // Referencia al audio actual para poder pararlo

    function stopAudio() {
        // Funci√≥n para interrumpir la reproducci√≥n (Barge-in)
        isPlaying = false;
        isAiSpeaking = false;
        audioQueue.length = 0; // Vaciar cola
        
        if (currentAudio) {
            currentAudio.pause();
            currentAudio.currentTime = 0;
            currentAudio = null;
        }
        
        statusText.innerText = "Interrumpido.";
        indicator.className = "indicator";
        
        // Asegurar que el micr√≥fono siga escuchando
        if (window.toggleRecognition) window.toggleRecognition(true);
    }

    ws.onmessage = (event) => {
        // Recibimos un chunk de audio (una frase)
        const blob = new Blob([event.data], { type: 'audio/wav' });
        const url = URL.createObjectURL(blob);
        
        audioQueue.push(url);
        
        if (!isPlaying) {
            statusText.innerText = "Sarah est√° hablando...";
            indicator.className = "indicator speaking";
            processQueue();
        }
    };

    async function processQueue() {
        if (audioQueue.length === 0) {
            isPlaying = false;
            // Peque√±o delay para evitar que el micro capte el eco final
            setTimeout(() => {
                if (audioQueue.length === 0 && !isPlaying) {
                    isAiSpeaking = false;
                    statusText.innerText = "Sarah te escucha.";
                    indicator.className = "indicator";
                    if (window.toggleRecognition) window.toggleRecognition(true);
                }
            }, 500);
            return;
        }
        
        // PAUSAR MICROFONO (Evitar que se escuche a s√≠ misma)
        if (!isAiSpeaking) {
            isAiSpeaking = true;
            if (window.toggleRecognition) window.toggleRecognition(false);
        }

        isPlaying = true;
        const url = audioQueue.shift();
        currentAudio = new Audio(url);
        
        currentAudio.onended = () => {
            URL.revokeObjectURL(url); // Liberar memoria
            currentAudio = null;
            processQueue(); // Reproducir siguiente
        };
        
        try {
            await currentAudio.play();
        } catch (e) {
            console.error("Error reproduciendo audio:", e);
            processQueue(); // Intentar siguiente si falla
        }
    }

    // 3. L√≥gica VAD y Web Speech API (H√≠brido)
    startBtn.onclick = async () => {
    try {
        statusText.innerText = "Configurando...";

        // Solicitar permisos de micr√≥fono primero
        console.log("üé§ Solicitando permisos de micr√≥fono...");
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            console.log("‚úÖ Permisos de micr√≥fono concedidos");
            // Detener el stream ya que Web Speech API lo manejar√°
            stream.getTracks().forEach(track => track.stop());
        } catch(permError) {
            console.error("‚ùå Error al solicitar permisos:", permError);
            statusText.innerText = "Permisos de micr√≥fono denegados";
            return;
        }

        // --- FASE 3: WEB SPEECH API CHECK ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const useWebSpeech = !!SpeechRecognition;

        if (useWebSpeech) {
            console.log("üöÄ Usando Web Speech API (Modo R√°pido)");
            const recognition = new SpeechRecognition();
            recognition.continuous = false; // Queremos frases cortas
            recognition.lang = 'en-US'; // O 'es-ES' y que el backend traduzca, pero 'en-US' fuerza pr√°ctica
            recognition.interimResults = false;
            recognition.maxAlternatives = 1;

            recognition.onstart = () => {
                console.log("üé§ Web Speech API iniciada");
                indicator.className = "indicator listening";
                statusText.innerText = "Escuchando (Nativo)...";
            };

            recognition.onend = () => {
                console.log("üõë Web Speech API terminada");
                // Reiniciar escucha siempre (Continuous listening), excepto si la IA est√° hablando
                if (!isAiSpeaking) {
                    console.log("üîÑ Reiniciando reconocimiento...");
                    try { recognition.start(); } catch(e) {
                        console.error("‚ùå Error al reiniciar:", e);
                    }
                }
            };

            recognition.onerror = (event) => {
                console.error("‚ùå Error Web Speech API:", event.error);
                statusText.innerText = `Error: ${event.error}`;
            };

            recognition.onresult = (event) => {
                const text = event.results[0][0].transcript;
                console.log("üó£Ô∏è Detectado:", text);
                console.log("üìä WebSocket estado:", ws.readyState, "(1 = OPEN)");

                // INTERRUPCI√ìN (Barge-in)
                stopAudio();

                indicator.className = "indicator processing";
                statusText.innerText = "Procesando...";

                if (ws.readyState === WebSocket.OPEN) {
                    console.log("üì§ Enviando mensaje:", {text: text});
                    ws.send(JSON.stringify({text: text}));
                    console.log("‚úÖ Mensaje enviado");
                    // El backend espera: {"text": ...} o bytes.
                    // ws.send(text) env√≠a string, el backend espera JSON parseable?
                    // Revisemos endpoints.py: await websocket.receive() devuelve dict si es json, o bytes/str.
                    // FastAPI websocket.receive() autodetecta. Si enviamos string JSON, lo parsea.
                    // Si enviamos texto plano, llega como 'text'. 
                    // endpoints.py verifica: if "text" in message.
                    // Si envio JSON string: ws.send(JSON.stringify({text: text})) -> message es dict {"text": text}
                    // Si envio raw text: ws.send(text) -> message es {"text": text} (FastAPI lo envuelve?)
                    // NO. websocket.receive() devuelve Message dictionary: {'type': 'websocket.receive', 'text': '...'}
                    // Mi codigo en endpoints.py hace: message = await websocket.receive().
                    // message es un dict con 'type', 'text' (si es texto) o 'bytes' (si es binario).
                    // Entonces: if "text" in message funciona para raw string frames.
                    // PERO, para ser consistentes con la logica de "text" vs "bytes", voy a enviar JSON string.
                }
            };
            
            // Fix para envio de JSON
            // La implementaci√≥n anterior enviaba ws.send(text).
            // FastAPI receive() -> {'type': 'websocket.receive', 'text': 'hola'}
            // Mi backend busca 'text' en el dict. ESTA BIEN.
            
            window.toggleRecognition = (enable) => {
                if (enable) {
                    try { recognition.start(); } catch(e) {}
                } else {
                    try { recognition.stop(); } catch(e) {}
                }
            };

            console.log("üöÄ Iniciando Web Speech Recognition...");
            try {
                recognition.start();
                startBtn.style.display = 'none';
                statusText.innerText = "Sarah lista (Modo R√°pido).";
                console.log("‚úÖ Recognition iniciado correctamente");
            } catch(e) {
                console.error("‚ùå Error al iniciar recognition:", e);
                statusText.innerText = "Error al iniciar reconocimiento";
            }

        } else {
            // --- FALLBACK: VAD + AUDIO RAW (Modo Lento) ---
            console.log("üê¢ Web Speech no soportado. Usando VAD + Audio Raw.");
            
            ort.env.wasm.numThreads = 1; 
            ort.env.wasm.proxy = false;
            ort.env.wasm.wasmPaths = "static/"; // Apuntar a local

            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                } 
            });

            const myvad = await vad.MicVAD.new({
                stream: stream,
                modelURL: "static/silero_vad.onnx", // Local
                
                positiveSpeechThreshold: 0.6,
                negativeSpeechThreshold: 0.4,
                minSpeechDurationFrames: 4,
                preSpeechPadFrames: 1,
                redemptionFrames: 15,
                
                onSpeechStart: () => {
                    console.log("VAD: Speech Start");
                    // INTERRUPCI√ìN (Barge-in)
                    stopAudio();
                    
                    indicator.className = "indicator listening";
                    statusText.innerText = "Escuchando...";
                },
                
                onSpeechEnd: (audio) => {
                    indicator.className = "indicator processing";
                    statusText.innerText = "Procesando...";
                    
                    const int16 = Int16Array.from(audio.map(n => n * 32767));
                    if (ws.readyState === WebSocket.OPEN) {
                        ws.send(int16.buffer); // ENVIAR BYTES
                    }
                }
            });

            myvad.start();
            startBtn.style.display = 'none';
            statusText.innerText = "Sarah lista (Modo VAD).";
            
            window.toggleRecognition = (enable) => {
                if (enable) myvad.start();
                else myvad.pause();
            };
        }

    } catch (err) {
        console.error("DETALLE DEL ERROR:", err);
        statusText.innerText = "Error de inicializaci√≥n.";
    }
    };
</script>

</body>
</html>
