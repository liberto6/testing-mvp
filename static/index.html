<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <title>English Academy AI - Streaming Pipeline</title>
    
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.19/dist/bundle.min.js"></script>

    <style>
        body { font-family: sans-serif; background: #121212; color: white; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        .card { background: #1e1e1e; padding: 40px; border-radius: 24px; text-align: center; width: 350px; border: 1px solid #333; }
        .indicator { width: 70px; height: 70px; border-radius: 50%; background: #333; margin: 20px auto; transition: 0.3s; border: 4px solid #121212; }
        .listening { background: #ff4b4b !important; box-shadow: 0 0 20px #ff4b4b; }
        .processing { background: #4b96ff !important; box-shadow: 0 0 20px #4b96ff; animation: pulse 1s infinite; }
        .speaking { background: #00e676 !important; box-shadow: 0 0 20px #00e676; animation: pulse 1s infinite; }
        @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } }
        button { padding: 15px 30px; border-radius: 50px; border: none; background: #00c853; color: white; font-weight: bold; cursor: pointer; width: 100%; }
        button:disabled { background: #444; cursor: not-allowed; }
        #status { margin-top: 20px; color: #aaa; font-size: 0.9rem; }
    </style>
</head>
<body>

<div class="card">
    <h1>Sarah AI</h1>
    <div id="indicator" class="indicator"></div>
    <p id="status">Cargando inteligencia...</p>
    <button id="startBtn" disabled>ESPERANDO SISTEMA...</button>
</div>

<script>
    const startBtn = document.getElementById('startBtn');
    const statusText = document.getElementById('status');
    const indicator = document.getElementById('indicator');

    // 1. Verificar carga de librer√≠as
    const checkLibs = setInterval(() => {
        if (typeof ort !== 'undefined' && typeof vad !== 'undefined') {
            clearInterval(checkLibs);
            startBtn.disabled = false;
            startBtn.innerText = "EMPEZAR CLASE";
            statusText.innerText = "Listo para conectar.";
        }
    }, 500);

    // 2. Conexi√≥n WebSocket
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const ws = new WebSocket(`${protocol}//${window.location.host}/ws`);

    ws.onopen = () => { statusText.innerText = "Conectado. Pulsa el bot√≥n."; };
    ws.onclose = () => { statusText.innerText = "Desconectado."; };

    // --- COLA DE AUDIO (Audio Queue) ---
    const audioQueue = [];
    let isPlaying = false;
    let currentAudio = null; // Referencia al audio actual para poder pararlo

    function stopAudio() {
        // Funci√≥n para interrumpir la reproducci√≥n (Barge-in)
        isPlaying = false;
        audioQueue.length = 0; // Vaciar cola
        
        if (currentAudio) {
            currentAudio.pause();
            currentAudio.currentTime = 0;
            currentAudio = null;
        }
        
        statusText.innerText = "Interrumpido.";
        indicator.className = "indicator";
        
        // Asegurar que el micr√≥fono siga escuchando
        if (window.toggleRecognition) window.toggleRecognition(true);
    }

    ws.onmessage = (event) => {
        // Recibimos un chunk de audio (una frase)
        const blob = new Blob([event.data], { type: 'audio/wav' });
        const url = URL.createObjectURL(blob);
        
        audioQueue.push(url);
        
        if (!isPlaying) {
            statusText.innerText = "Sarah est√° hablando...";
            indicator.className = "indicator speaking";
            processQueue();
        }
    };

    async function processQueue() {
        if (audioQueue.length === 0) {
            isPlaying = false;
            statusText.innerText = "Sarah te escucha.";
            indicator.className = "indicator";
            return;
        }
        
        // NO PAUSAR MICROFONO (Full Duplex)
        // if (!isPlaying) {
        //      if (window.toggleRecognition) window.toggleRecognition(false);
        // }

        isPlaying = true;
        const url = audioQueue.shift();
        currentAudio = new Audio(url);
        
        currentAudio.onended = () => {
            URL.revokeObjectURL(url); // Liberar memoria
            currentAudio = null;
            processQueue(); // Reproducir siguiente
        };
        
        try {
            await currentAudio.play();
        } catch (e) {
            console.error("Error reproduciendo audio:", e);
            processQueue(); // Intentar siguiente si falla
        }
    }

    // 3. L√≥gica VAD y Web Speech API (H√≠brido)
    startBtn.onclick = async () => {
    try {
        statusText.innerText = "Configurando...";
        
        // --- FASE 3: WEB SPEECH API CHECK ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const useWebSpeech = !!SpeechRecognition;
        
        if (useWebSpeech) {
            console.log("üöÄ Usando Web Speech API (Modo R√°pido)");
            const recognition = new SpeechRecognition();
            recognition.continuous = false; // Queremos frases cortas
            recognition.lang = 'en-US'; // O 'es-ES' y que el backend traduzca, pero 'en-US' fuerza pr√°ctica
            recognition.interimResults = false;
            recognition.maxAlternatives = 1;

            recognition.onstart = () => {
                indicator.className = "indicator listening";
                statusText.innerText = "Escuchando (Nativo)...";
            };

            recognition.onend = () => {
                // Reiniciar escucha siempre (Continuous listening)
                try { recognition.start(); } catch(e) {}
            };

            recognition.onresult = (event) => {
                const text = event.results[0][0].transcript;
                console.log("üó£Ô∏è Detectado:", text);
                
                // INTERRUPCI√ìN (Barge-in)
                stopAudio();
                
                indicator.className = "indicator processing";
                statusText.innerText = "Procesando...";
                
                if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({text: text})); // Enviar como JSON expl√≠cito si se prefiere, pero el backend espera JSON o Bytes
                    // El backend espera: {"text": ...} o bytes.
                    // ws.send(text) env√≠a string, el backend espera JSON parseable?
                    // Revisemos endpoints.py: await websocket.receive() devuelve dict si es json, o bytes/str.
                    // FastAPI websocket.receive() autodetecta. Si enviamos string JSON, lo parsea.
                    // Si enviamos texto plano, llega como 'text'. 
                    // endpoints.py verifica: if "text" in message.
                    // Si envio JSON string: ws.send(JSON.stringify({text: text})) -> message es dict {"text": text}
                    // Si envio raw text: ws.send(text) -> message es {"text": text} (FastAPI lo envuelve?)
                    // NO. websocket.receive() devuelve Message dictionary: {'type': 'websocket.receive', 'text': '...'}
                    // Mi codigo en endpoints.py hace: message = await websocket.receive().
                    // message es un dict con 'type', 'text' (si es texto) o 'bytes' (si es binario).
                    // Entonces: if "text" in message funciona para raw string frames.
                    // PERO, para ser consistentes con la logica de "text" vs "bytes", voy a enviar JSON string.
                }
            };
            
            // Fix para envio de JSON
            // La implementaci√≥n anterior enviaba ws.send(text).
            // FastAPI receive() -> {'type': 'websocket.receive', 'text': 'hola'}
            // Mi backend busca 'text' en el dict. ESTA BIEN.
            
            window.toggleRecognition = (enable) => {
                if (enable) {
                    try { recognition.start(); } catch(e) {}
                } else {
                    try { recognition.stop(); } catch(e) {}
                }
            };

            recognition.start();
            startBtn.style.display = 'none';
            statusText.innerText = "Sarah lista (Modo R√°pido).";

        } else {
            // --- FALLBACK: VAD + AUDIO RAW (Modo Lento) ---
            console.log("üê¢ Web Speech no soportado. Usando VAD + Audio Raw.");
            
            ort.env.wasm.numThreads = 1; 
            ort.env.wasm.proxy = false;
            ort.env.wasm.wasmPaths = "static/"; // Apuntar a local

            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    channelCount: 1,
                    echoCancellation: true,
                    autoGainControl: true,
                    noiseSuppression: true,
                    // Flags espec√≠ficos de Chrome/WebRTC para AEC agresivo
                    googEchoCancellation: true,
                    googAutoGainControl: true,
                    googNoiseSuppression: true,
                    googHighpassFilter: true
                } 
            });

            // --- ANALIZADOR DE ENERG√çA (Barge-in por Volumen) ---
            // Usamos esto para detectar interrupciones fuertes (gritos/habla) incluso si el VAD est√° "cegado" por el eco.
            const audioContext = new AudioContext();
            const source = audioContext.createMediaStreamSource(stream);
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            source.connect(analyser);
            
            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            
            // Loop de monitoreo de volumen
            function checkVolume() {
                if (!isPlaying) {
                    requestAnimationFrame(checkVolume);
                    return;
                }
                
                analyser.getByteFrequencyData(dataArray);
                
                // Calcular volumen promedio (RMS aprox)
                let sum = 0;
                for (let i = 0; i < dataArray.length; i++) {
                    sum += dataArray[i];
                }
                const average = sum / dataArray.length;
                
                // UMBRAL DE BARGE-IN (Ajustable)
                // Si el volumen supera ~20% (50/255), asumimos que el usuario est√° hablando fuerte sobre la IA.
                // El eco cancelado por AEC deber√≠a ser mucho menor a esto.
                if (average > 40) { 
                    console.log(`üîä VOLUMEN ALTO (${average.toFixed(0)}) -> BARGE-IN DETECTADO`);
                    stopAudio();
                }
                
                requestAnimationFrame(checkVolume);
            }
            checkVolume();


            const myvad = await vad.MicVAD.new({
                stream: stream,
                modelURL: "static/silero_vad.onnx", // Local
                
                // AJUSTES VAD (M√°s relajados ahora que tenemos Gating)
                positiveSpeechThreshold: 0.8,
                negativeSpeechThreshold: 0.6,    
                minSpeechDurationFrames: 4,      
                preSpeechPadFrames: 3,           
                redemptionFrames: 15,            
                
                onSpeechStart: () => {
                    // --- VAD GATING (Anti-Eco) ---
                    // Si la IA habla, IGNORAMOS el VAD completamente.
                    // Confiamos en el AnalyserNode (arriba) para detectar interrupciones reales por volumen.
                    if (isPlaying) {
                        console.log("üõ°Ô∏è VAD ignorado durante reproducci√≥n (Eco Protection).");
                        return;
                    }
                    
                    console.log("VAD: Speech Start");
                    indicator.className = "indicator listening";
                    statusText.innerText = "Escuchando...";
                },
                
                onSpeechEnd: (audio) => {
                    // --- VAD GATING (Anti-Eco) ---
                    if (isPlaying) {
                        console.log("ÔøΩÔ∏è Audio descartado durante reproducci√≥n.");
                        return;
                    }

                    console.log("VAD: Speech End");
                    indicator.className = "indicator processing";
                    statusText.innerText = "Procesando...";
                    
                    const int16 = Int16Array.from(audio.map(n => n * 32767));
                    if (ws.readyState === WebSocket.OPEN) {
                        ws.send(int16.buffer); // ENVIAR BYTES
                    }
                }
            });

            myvad.start();
            startBtn.style.display = 'none';
            statusText.innerText = "Sarah lista (Modo VAD).";
            
            window.toggleRecognition = (enable) => {
                if (enable) myvad.start();
                else myvad.pause();
            };
        }

    } catch (err) {
        console.error("DETALLE DEL ERROR:", err);
        statusText.innerText = "Error de inicializaci√≥n.";
    }
    };
</script>

</body>
</html>
