"""
Main Entry Point for Pipecat Voice Pipeline
FastAPI server with WebSocket support
"""

import sys
import os
import asyncio
import logging
import time
import numpy as np
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles

from src.utils.config import ConfigManager, get_config_manager
from src.utils.gpu_utils import gpu_manager
from src.pipeline import SimplifiedVoicePipeline
from src.processors.stt_whisper_gpu import WhisperGPUConfig
from src.processors.llm_groq import GroqConfig
from src.processors.tts_kokoro import KokoroConfig
from src.processors.tts_edge import EdgeTTSConfig

from pipecat.frames.frames import AudioRawFrame, TextFrame

logger = logging.getLogger(__name__)

# Global pipeline instance
pipeline: SimplifiedVoicePipeline = None
config_manager: ConfigManager = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup/shutdown"""

    global pipeline, config_manager

    # --- Startup ---
    logger.info("üöÄ Starting Pipecat Voice Pipeline Server...")

    # Load configuration
    config_path = os.getenv("CONFIG_PATH", None)  # Check environment variable
    if config_path:
        logger.info(f"üìã Using config from: {config_path}")

    config_manager = get_config_manager(config_path=config_path, auto_optimize=True)
    config_manager.print_config()

    # Initialize pipeline
    pipeline = SimplifiedVoicePipeline(config=config_manager.get_config())

    logger.info("‚úÖ Pipeline ready")

    # Print GPU info
    if gpu_manager.device.type == "cuda":
        stats = gpu_manager.get_memory_stats()
        logger.info(f"üéÆ GPU Stats: {stats}")

    yield

    # --- Shutdown ---
    logger.info("üõë Shutting down server...")

    # Cleanup
    if gpu_manager.device.type == "cuda":
        gpu_manager.clear_cache()

    logger.info("‚úÖ Shutdown complete")


# Create FastAPI app
app = FastAPI(
    title="Pipecat Voice Pipeline",
    description="GPU-Optimized Voice Conversation Pipeline",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "name": "Pipecat Voice Pipeline",
        "version": "1.0.0",
        "status": "running",
        "gpu_enabled": gpu_manager.device.type == "cuda"
    }


@app.get("/health")
async def health():
    """Health check endpoint"""

    health_status = {
        "status": "healthy",
        "gpu": {
            "available": gpu_manager.device.type == "cuda",
            "device": str(gpu_manager.device)
        }
    }

    if gpu_manager.device.type == "cuda":
        health_status["gpu"]["memory"] = gpu_manager.get_memory_stats()

    return health_status


@app.get("/config")
async def get_config():
    """Get current configuration"""

    if config_manager:
        return {
            "stt": {
                "model": config_manager.config.stt.model,
                "device": config_manager.config.stt.device,
                "compute_type": config_manager.config.stt.compute_type
            },
            "llm": {
                "provider": config_manager.config.llm.provider,
                "model": config_manager.config.llm.model
            },
            "tts": {
                "provider": config_manager.config.tts.provider,
                "device": config_manager.config.tts.device,
                "voice": config_manager.config.tts.voice
            }
        }

    return {"error": "Configuration not loaded"}


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket endpoint for voice conversation

    Maintains compatibility with existing frontend
    """

    await websocket.accept()
    logger.info("üîå WebSocket connected")

    # Track current processing task for interruption handling
    current_task = None

    try:
        while True:
            try:
                # Receive message from client
                message = await websocket.receive()
                logger.info(f"üì• Received message type: {message.get('type')}")
                logger.debug(f"üì• Full message: {message}")

                # Check if it's a disconnect message
                if message.get("type") == "websocket.disconnect":
                    logger.info("üîå Client initiated disconnect")
                    break

                # Cancel previous task if still running (barge-in)
                if current_task and not current_task.done():
                    logger.info("‚úã Interruption detected, canceling previous response...")
                    current_task.cancel()
                    try:
                        await current_task
                    except asyncio.CancelledError:
                        pass

                # Create new processing task
                current_task = asyncio.create_task(
                    process_message(websocket, message)
                )

            except RuntimeError as e:
                if "Cannot call" in str(e) and "disconnect" in str(e):
                    logger.info("üîå WebSocket already disconnected")
                    break
                raise

    except WebSocketDisconnect:
        logger.info("üîå WebSocket disconnected")
        if current_task:
            current_task.cancel()

    except Exception as e:
        logger.error(f"‚ùå WebSocket error: {e}", exc_info=True)
        if current_task:
            current_task.cancel()

    finally:
        if current_task and not current_task.done():
            current_task.cancel()
        logger.info("üßπ WebSocket cleanup complete")


async def process_message(websocket: WebSocket, message: dict):
    """Process a single message through the pipeline"""

    try:
        user_text = ""
        t_start = time.time()

        # Handle text input (Web Speech API)
        if "text" in message:
            # Check if it's a JSON string that needs parsing
            text_content = message["text"]

            # Try to parse as JSON first (frontend sends JSON.stringify({text: ...}))
            try:
                import json
                parsed = json.loads(text_content)
                if isinstance(parsed, dict) and "text" in parsed:
                    user_text = parsed["text"]
                else:
                    user_text = text_content
            except (json.JSONDecodeError, ValueError):
                # Not JSON, use as-is
                user_text = text_content

            logger.info(f"üì® Received text: '{user_text}'")

        # Handle audio input (Whisper STT)
        elif "bytes" in message:
            audio_bytes = message["bytes"]

            if len(audio_bytes) == 0:
                return

            # Convert to numpy for silence detection
            audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0

            # Check for silence
            amplitude = np.max(np.abs(audio_np))
            if amplitude < 0.01:
                logger.debug("üîá Silence detected, ignoring")
                return

            logger.info("üé§ Processing audio...")

            # Run STT
            t_stt_start = time.time()
            user_text = await pipeline.process_audio(audio_bytes)
            t_stt = time.time() - t_stt_start

            if not user_text:
                logger.warning("‚ö†Ô∏è STT returned no text")
                return

            logger.info(f"üìù Transcribed: '{user_text}' ({t_stt:.2f}s)")

        if not user_text:
            return

        # NO enviar response_start - el frontend original no lo espera
        # await websocket.send_json({"type": "response_start"})

        # Stream LLM -> TTS pipeline
        first_audio_sent = False
        t_first_byte = 0

        async for sentence in pipeline.stream_llm_response(user_text):
            logger.info(f"ü§ñ LLM: '{sentence[:30]}...'")

            # Generate TTS audio
            t_tts_start = time.time()
            audio_bytes = await pipeline.process_text_to_speech(sentence)
            t_tts = time.time() - t_tts_start

            if audio_bytes:
                # Send audio directly as bytes (WAV format)
                # Frontend expects raw audio bytes, not wrapped in JSON
                await websocket.send_bytes(audio_bytes)

                if not first_audio_sent:
                    t_first_byte = time.time() - t_start
                    logger.info(f"‚ö° Time to first audio: {t_first_byte:.2f}s")
                    first_audio_sent = True

                logger.info(f"üîä TTS: {len(sentence)} chars in {t_tts:.2f}s")

        # Total latency
        t_total = time.time() - t_start
        logger.info(f"‚úÖ Total latency: {t_total:.2f}s")

    except asyncio.CancelledError:
        logger.info("üõë Processing canceled (user interrupted)")
        raise

    except Exception as e:
        logger.error(f"‚ùå Processing error: {e}", exc_info=True)


# Serve static files (for frontend)
STATIC_DIR = Path(__file__).parent.parent / "static"
if STATIC_DIR.exists():
    app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")

    # Serve frontend files
    @app.get("/index.html")
    async def serve_index():
        return FileResponse(STATIC_DIR / "index.html")

    @app.get("/test.html")
    async def serve_test():
        return FileResponse(STATIC_DIR / "test.html")


if __name__ == "__main__":
    import uvicorn

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Run server
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # Disable reload in production
        log_level="info"
    )
