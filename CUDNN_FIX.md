# Soluciones para Error de cuDNN

## ‚ùå Error
```
Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}
Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor
Aborted (core dumped)
```

Este error ocurre porque `faster-whisper` necesita cuDNN y puede no estar compatible con tu versi√≥n.

---

## üöÄ Soluci√≥n 1: Usar CPU para STT (M√°s R√°pido de Implementar)

### Opci√≥n A: Usar configuraci√≥n CPU

```bash
# Usar config con STT en CPU
export CONFIG_PATH=configs/runpod_cpu_stt.yaml
python run.py
```

### Opci√≥n B: Modificar config manualmente

Edita tu `.env`:
```bash
# Forzar CPU para STT
export WHISPER_MODEL=base
export FORCE_CPU=true
```

**Ventajas:**
- ‚úÖ Funciona inmediatamente
- ‚úÖ No requiere arreglar cuDNN
- ‚úÖ TTS puede seguir usando GPU

**Desventajas:**
- ‚ö†Ô∏è STT ser√° m√°s lento (~200-300ms en lugar de ~90ms)
- ‚ö†Ô∏è Solo puede usar modelos peque√±os (tiny/base/small)

---

## üîß Soluci√≥n 2: Arreglar cuDNN (Recomendado para Producci√≥n)

### Paso 1: Instalar cuDNN

```bash
# Ejecutar script de fix
./fix_cudnn.sh

# O manualmente:
pip install nvidia-cudnn-cu12
```

### Paso 2: Configurar LD_LIBRARY_PATH

```bash
# Encontrar cuDNN
find /usr -name "libcudnn*.so*" 2>/dev/null

# Agregar al PATH (reemplaza con tu ruta)
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Hacer permanente
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

### Paso 3: Verificar

```bash
# Test r√°pido
python -c "from faster_whisper import WhisperModel; print('‚úÖ cuDNN OK')"

# Si funciona, iniciar servidor
python run.py
```

---

## üîÑ Soluci√≥n 3: Usar OpenAI Whisper (Alternativa)

OpenAI Whisper es m√°s compatible que faster-whisper pero un poco m√°s lento.

### Instalar

```bash
pip install openai-whisper
```

### Modificar c√≥digo

Edita `src/pipeline.py` y cambia:

```python
# ANTES:
from src.processors.stt_whisper_gpu import WhisperGPUProcessor, WhisperGPUConfig

# DESPU√âS:
from src.processors.stt_whisper_openai import OpenAIWhisperProcessor, OpenAIWhisperConfig
```

Y en el init:

```python
# ANTES:
self.stt_processor = WhisperGPUProcessor(
    config=WhisperGPUConfig(...)
)

# DESPU√âS:
self.stt_processor = OpenAIWhisperProcessor(
    config=OpenAIWhisperConfig(
        model_size=self.config.stt.model,
        device=self.config.stt.device,
        language=self.config.stt.language
    )
)
```

---

## üê≥ Soluci√≥n 4: Usar Docker con cuDNN Incluido

Si est√°s en RunPod, usa el Dockerfile que incluye cuDNN:

```bash
# Build con cuDNN incluido
docker build -f deployment/runpod/Dockerfile -t pipecat-voice .

# Run
docker run --gpus all -p 8000:8000 \
  -e GROQ_API_KEY=$GROQ_API_KEY \
  pipecat-voice
```

---

## üß™ Diagn√≥stico

### Verificar CUDA

```bash
nvidia-smi
nvcc --version
```

### Verificar cuDNN

```bash
# Buscar librer√≠as
ldconfig -p | grep cudnn

# Buscar archivos
find /usr -name "libcudnn*.so*" 2>/dev/null

# Python check
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

### Verificar faster-whisper

```bash
python -c "from faster_whisper import WhisperModel; model = WhisperModel('tiny', device='cuda'); print('OK')"
```

---

## ‚úÖ Recomendaci√≥n por Escenario

### Testing/Desarrollo R√°pido
‚Üí **Soluci√≥n 1**: Usar CPU para STT
```bash
export CONFIG_PATH=configs/runpod_cpu_stt.yaml
python run.py
```

### Producci√≥n con RunPod
‚Üí **Soluci√≥n 2**: Arreglar cuDNN
```bash
./fix_cudnn.sh
python run.py
```

### M√°xima Compatibilidad
‚Üí **Soluci√≥n 3**: Usar OpenAI Whisper
```bash
pip install openai-whisper
# Modificar src/pipeline.py seg√∫n instrucciones arriba
python run.py
```

---

## üìä Comparaci√≥n de Performance

| Soluci√≥n | Latencia STT | Calidad | Complejidad |
|----------|--------------|---------|-------------|
| CPU STT (base) | ~200ms | Media | ‚≠ê F√°cil |
| faster-whisper GPU | ~90ms | Alta | ‚≠ê‚≠ê‚≠ê Complejo |
| openai-whisper GPU | ~120ms | Alta | ‚≠ê‚≠ê Medio |

---

## üéØ Mi Recomendaci√≥n

Para empezar **ahora mismo**:

```bash
# 1. Usar config CPU (funciona siempre)
export CONFIG_PATH=configs/runpod_cpu_stt.yaml
python run.py

# 2. En paralelo, arreglar cuDNN
./fix_cudnn.sh

# 3. Cuando cuDNN funcione, volver a config GPU
python run.py  # Sin CONFIG_PATH, usar√° auto-detecci√≥n
```

---

## üí° Tips Adicionales

1. **Kokoro TTS puede seguir en GPU** incluso si STT est√° en CPU
2. **El performance general seguir√° siendo bueno** (LLM es el componente m√°s lento)
3. **CPU STT con modelo "base" es aceptable** para desarrollo
4. **Para producci√≥n, vale la pena arreglar cuDNN** para usar GPU

---

## üìû Siguiente Paso

Ejecuta esto AHORA para iniciar con CPU STT:

```bash
cd /workspace/testing-mvp
export CONFIG_PATH=configs/runpod_cpu_stt.yaml
python run.py
```

Deber√≠as ver el servidor iniciando sin el error de cuDNN! üöÄ
